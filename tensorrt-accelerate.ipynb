{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<table><tr><td bgcolor=#000000>\n",
    "<p align='left'>\n",
    "<font size=6 color=white><b>Before:</b></font><br><br>\n",
    "</p>\n",
    "<img src=\"https://sias.uestc.edu.cn/images/logo.png\" align=center hspace=250><br><br>\n",
    "\n",
    "<font face=宋体 color=white size=5><center><b>\n",
    "    本项目由电子科大深圳高研院-机器学习课程军事飞机识别课题组所有。<br/>\n",
    "    欢迎交流指正！</font>\n",
    "    \n",
    "<font face=TimesNewRoman size=2 color=lightblue><center><b>\n",
    "    This project is base on UESTC-SIAS ML Military-Detect Research Group.<br/>\n",
    "    Welcome communicate with us and correct our mistakes!\n",
    "    \n",
    "</td></tr></table>\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4>  \n",
    "**Attention: before runnnig these code, change seeting->Accelerator->GPU**\n",
    "\n",
    "    \n",
    "**If you need cuda version 11.0, change seeting->Environment->Always use latest environment**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0870f1ac-13f5-4b33-a65a-7bd53cd523b9",
    "_uuid": "12fcd606-1c26-46a7-867d-ab78b4dfb2b7"
   },
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T11:32:23.058377Z",
     "iopub.status.busy": "2021-11-04T11:32:23.058061Z",
     "iopub.status.idle": "2021-11-04T11:32:23.09042Z",
     "shell.execute_reply": "2021-11-04T11:32:23.089606Z",
     "shell.execute_reply.started": "2021-11-04T11:32:23.058344Z"
    }
   },
   "outputs": [],
   "source": [
    "import pycuda.autoinit \n",
    "import pycuda.driver as cuda \n",
    "\n",
    "(free,total)=cuda.mem_get_info() \n",
    "print(\"Global memory occupancy:%f%% free\"%(free*100/total)) \n",
    "\n",
    "for devicenum in range(cuda.Device.count()): \n",
    "    device=cuda.Device(devicenum) \n",
    "    attrs=device.get_attributes() \n",
    "\n",
    "    #Beyond this point is just pretty printing \n",
    "    print(\"\\n===Attributes for device %d: \"%devicenum + str(device.name())) \n",
    "    for (key,value) in attrs.items(): \n",
    "        print(str(key)+':'+f\"\\033[1;31m{str(value)}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T11:30:57.022723Z",
     "iopub.status.busy": "2021-11-04T11:30:57.021973Z",
     "iopub.status.idle": "2021-11-04T11:30:57.710239Z",
     "shell.execute_reply": "2021-11-04T11:30:57.70944Z",
     "shell.execute_reply.started": "2021-11-04T11:30:57.022685Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0807d02-cefb-441a-9ce0-c20bf1b465e6",
    "_uuid": "44e1ee28-50f3-459b-8209-0165b11f9973",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:40:01.469681Z",
     "iopub.status.busy": "2021-11-03T11:40:01.468857Z",
     "iopub.status.idle": "2021-11-03T11:40:02.299926Z",
     "shell.execute_reply": "2021-11-03T11:40:02.299145Z",
     "shell.execute_reply.started": "2021-11-03T11:40:01.469531Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b398a5c-7990-44d4-8041-8ded64179775",
    "_uuid": "9fac6f3a-8b37-471a-a0ee-2e9cf0f5fd02",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!nvcc -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "97e9b600-196c-4ec8-827c-98fcdd2dfce9",
    "_uuid": "e0ef9526-adde-4840-8a93-e4b985bd9306"
   },
   "source": [
    "# Install TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65caca05-358d-4df2-bfe1-6dbfde0a3911",
    "_uuid": "1b51f5f6-6ce9-46b3-abd0-0ad0fc8ee2c4",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:40:02.968137Z",
     "iopub.status.busy": "2021-11-03T11:40:02.967874Z",
     "iopub.status.idle": "2021-11-03T11:40:59.28831Z",
     "shell.execute_reply": "2021-11-03T11:40:59.287505Z",
     "shell.execute_reply.started": "2021-11-03T11:40:02.968102Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# install basic packages\n",
    "!apt-get update && \\\n",
    "    apt-get install -y \\\n",
    "        libnvonnxparsers8=8.0.3-1+cuda11.3 \\\n",
    "        libnvonnxparsers-dev=8.0.3-1+cuda11.3 \\\n",
    "        libnvparsers8=8.0.3-1+cuda11.3\\\n",
    "        libnvparsers-dev=8.0.3-1+cuda11.3 \\\n",
    "        libnvinfer8=8.0.3-1+cuda11.3\\\n",
    "        libnvinfer-plugin8=8.0.3-1+cuda11.3\\\n",
    "        libnvinfer-plugin-dev=8.0.3-1+cuda11.3 \\\n",
    "        libnvinfer-dev=8.0.3-1+cuda11.3 \\\n",
    "        python3-libnvinfer=8.0.3-1+cuda11.3 \\\n",
    "        python3-libnvinfer-dev=8.0.3-1+cuda11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4e6271ed-e465-4f88-9cc9-8d1059b31187",
    "_uuid": "3cedd3b0-777c-4436-bf9c-dc114dac7659",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:40:59.291251Z",
     "iopub.status.busy": "2021-11-03T11:40:59.290977Z",
     "iopub.status.idle": "2021-11-03T11:42:17.311047Z",
     "shell.execute_reply": "2021-11-03T11:42:17.309959Z",
     "shell.execute_reply.started": "2021-11-03T11:40:59.291214Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# unzip deb file and install \n",
    "!dpkg -i ../input/tensorrt/tensorrt.deb && \\\n",
    "    apt-key add /var/nv-tensorrt-repo-ubuntu1804-cuda11.3-trt8.0.3.4-ga-20210831/7fa2af80.pub && \\\n",
    "    apt-get update && \\\n",
    "    apt-get install -y tensorrt # uff-converter-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "88615c80-b391-44f8-9625-55a0d09fd539",
    "_uuid": "bfc1472e-4aae-4dd2-a143-e60467b8e7ed",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:42:17.313384Z",
     "iopub.status.busy": "2021-11-03T11:42:17.313088Z",
     "iopub.status.idle": "2021-11-03T11:42:23.108045Z",
     "shell.execute_reply": "2021-11-03T11:42:23.107173Z",
     "shell.execute_reply.started": "2021-11-03T11:42:17.313342Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# install uff and onnx \n",
    "!apt-get install uff-converter-tf -y\n",
    "!apt-get install onnx-graphsurgeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2fc3d0f4-c02a-4f11-bb8b-3d9020d2d886",
    "_uuid": "444b32c6-ee13-4303-b924-c4a27497aae6",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:42:23.111027Z",
     "iopub.status.busy": "2021-11-03T11:42:23.110743Z",
     "iopub.status.idle": "2021-11-03T11:42:23.820152Z",
     "shell.execute_reply": "2021-11-03T11:42:23.819145Z",
     "shell.execute_reply.started": "2021-11-03T11:42:23.110989Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# view tensorrt-pkgs install condition\n",
    "!dpkg -l | grep TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9eef231-f707-49a7-8900-6b5702695b36",
    "_uuid": "55664d19-8644-47fc-bbbd-eba4275c0760",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:42:23.822202Z",
     "iopub.status.busy": "2021-11-03T11:42:23.821934Z",
     "iopub.status.idle": "2021-11-03T11:42:40.338074Z",
     "shell.execute_reply": "2021-11-03T11:42:40.337225Z",
     "shell.execute_reply.started": "2021-11-03T11:42:23.822165Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# install/update basic repo\n",
    "# !python3 -m pip install --upgrade setuptools pip\n",
    "!python3 -m pip install nvidia-pyindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "88b6190c-4a52-49a1-b83c-c7c37d4665d4",
    "_uuid": "21a9da91-ee28-4596-9b8b-76cb0568a6b5",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:42:40.34011Z",
     "iopub.status.busy": "2021-11-03T11:42:40.339768Z",
     "iopub.status.idle": "2021-11-03T11:44:09.07797Z",
     "shell.execute_reply": "2021-11-03T11:44:09.076962Z",
     "shell.execute_reply.started": "2021-11-03T11:42:40.340069Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# install tensorrt(python3)\n",
    "!python3 -m pip install nvidia-tensorrt==8.0.3.4\n",
    "!python3 -m pip install uff graphsurgeon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6fb2dfc4-d42a-4f1c-87a6-74b51992a5fa",
    "_uuid": "1cf4b3c3-71c5-4e8e-9099-b4a506de4a00"
   },
   "source": [
    "# Validate Python Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b46a60fd-5dc5-444b-ada2-f64117f2b827",
    "_uuid": "0a79de8d-2d9b-4713-b652-a778c80cf765",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:44:09.081795Z",
     "iopub.status.busy": "2021-11-03T11:44:09.081569Z",
     "iopub.status.idle": "2021-11-03T11:44:13.347119Z",
     "shell.execute_reply": "2021-11-03T11:44:13.346241Z",
     "shell.execute_reply.started": "2021-11-03T11:44:09.081767Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import ctypes\n",
    "import uff\n",
    "import graphsurgeon as gs\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "\n",
    "print(trt.__version__)\n",
    "assert trt.Builder(trt.Logger())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3387479f-e5bc-4329-b64a-30030598f608",
    "_uuid": "04707287-df3f-4488-83d9-fe4fce8c3be6"
   },
   "source": [
    "# Install Tensorrtx and yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43da5cbc-b558-430f-b776-8bd8b58657a5",
    "_uuid": "9459ef01-f6d9-49e4-8c1d-2d65695da75f",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:44:13.350919Z",
     "iopub.status.busy": "2021-11-03T11:44:13.350636Z",
     "iopub.status.idle": "2021-11-03T11:44:29.972443Z",
     "shell.execute_reply": "2021-11-03T11:44:29.971597Z",
     "shell.execute_reply.started": "2021-11-03T11:44:13.35088Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working\n",
    "!git clone https://github.com/wang-xinyu/tensorrtx.git\n",
    "!git clone -b v6.0 https://github.com/ultralytics/yolov5.git\n",
    "!pip install -r ./yolov5/requirements.txt\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "936253f5-4d65-4397-a563-305c3d05e283",
    "_uuid": "e211c488-1f15-4cb7-9fdf-c50bc8a24eb8",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:44:29.975598Z",
     "iopub.status.busy": "2021-11-03T11:44:29.974427Z",
     "iopub.status.idle": "2021-11-03T11:44:32.800645Z",
     "shell.execute_reply": "2021-11-03T11:44:32.799791Z",
     "shell.execute_reply.started": "2021-11-03T11:44:29.975565Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# solve \"ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26‘ not found\"\n",
    "!cp /opt/conda/pkgs/libstdcxx-ng-11.2.0-he4da1e4_8/lib/libstdc++.so.6.0.29 /usr/lib/x86_64-linux-gnu/\n",
    "!rm /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
    "!ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.29 /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n",
    "!strings /opt/conda/pkgs/libstdcxx-ng-11.2.0-he4da1e4_8/lib/libstdc++.so.6.0.29 | grep GLIBCXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f9cb1597-6f0c-4d62-9182-c2651db711f0",
    "_uuid": "81d1763f-0495-4e03-9801-189efa513dbd"
   },
   "source": [
    "# Convert weights and detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f6ab833-5370-4cfa-82c4-bd4427eaff85",
    "_uuid": "5c58ebfa-73d7-4591-80d3-009666c0adea"
   },
   "source": [
    "### Generate .wts from pytorch with .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b820c4e-09c7-46a3-95d7-2c04776bf3d1",
    "_uuid": "8cebbc26-e6aa-4770-b631-e4dd088735fb",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:44:32.802296Z",
     "iopub.status.busy": "2021-11-03T11:44:32.802028Z",
     "iopub.status.idle": "2021-11-03T11:44:58.098422Z",
     "shell.execute_reply": "2021-11-03T11:44:58.097461Z",
     "shell.execute_reply.started": "2021-11-03T11:44:32.802244Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "clone code according to above Different versions of yolov5\n",
    "download https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt\n",
    "'''\n",
    "%cd /kaggle/working\n",
    "!cp /kaggle/input/tensorrt/gen_wts.py  ./yolov5\n",
    "!cp /kaggle/input/weights/exp/weights/best.pt ./yolov5\n",
    "!cp ./yolov5s.pt ./yolov5\n",
    "\n",
    "'''\n",
    "a file 'yolov5s.wts' will be generated.\n",
    "'''\n",
    "%cd ./yolov5\n",
    "!python ./gen_wts.py -w best.pt -of yolov5s.pt -o ../best.wts\n",
    "!cp ../best.wts ../tensorrtx/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d24fced-370f-47f2-a915-b124c29bbb8a",
    "_uuid": "50dc30b8-aa32-44fa-b4b6-98d3e92828f4"
   },
   "source": [
    "### Build tensorrtx/yolov5 and create Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7365038-d468-47df-8b0a-a3213d76383b",
    "_uuid": "f1eff2e7-cb0c-46a6-9f9f-28e4e012ccf3",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:44:58.109874Z",
     "iopub.status.busy": "2021-11-03T11:44:58.109615Z",
     "iopub.status.idle": "2021-11-03T11:45:21.48685Z",
     "shell.execute_reply": "2021-11-03T11:45:21.486029Z",
     "shell.execute_reply.started": "2021-11-03T11:44:58.109838Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working\n",
    "# # // update CLASS_NUM in yololayer.h if your model is trained on custom dataset\n",
    "!rm -rf ./tensorrtx/yolov5/build/\n",
    "!mkdir -p ./tensorrtx/yolov5/build\n",
    "!cp /kaggle/input/tensorrt/yololayer.h /kaggle/working/tensorrtx/yolov5/\n",
    "!cp /kaggle/input/tensorrt/yolov5.cpp /kaggle/working/tensorrtx/yolov5/\n",
    "\n",
    "%cd ./tensorrtx/yolov5/build\n",
    "!cmake ..\n",
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a89b96e-a08f-4998-9f56-52ba96fd240c",
    "_uuid": "37ff906d-3c2b-4ab8-8156-a663e470792a",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:45:21.514935Z",
     "iopub.status.busy": "2021-11-03T11:45:21.514614Z",
     "iopub.status.idle": "2021-11-03T11:47:30.99898Z",
     "shell.execute_reply": "2021-11-03T11:47:30.99813Z",
     "shell.execute_reply.started": "2021-11-03T11:45:21.514898Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/tensorrtx/yolov5/build\n",
    "!apt-get install sudo\n",
    "!sudo ./yolov5 -s ../best.wts /kaggle/working/best.engine s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c09af641-2572-4c33-8dfc-4ac83996053f",
    "_uuid": "c36af203-9bff-4c5e-8725-b2bd045cf284",
    "execution": {
     "iopub.execute_input": "2021-11-03T11:47:31.007159Z",
     "iopub.status.busy": "2021-11-03T11:47:31.00661Z",
     "iopub.status.idle": "2021-11-03T11:47:48.380366Z",
     "shell.execute_reply": "2021-11-03T11:47:48.379436Z",
     "shell.execute_reply.started": "2021-11-03T11:47:31.007119Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install yt-dlp\n",
    "%cd /kaggle/working\n",
    "!yt-dlp -o \"001.mp4\" -f \"bestvideo[ext=mp4]\" \"https://www.youtube.com/watch?v=EgjWQK3QmWg\"\n",
    "# https://www.youtube.com/watch?v=Drl4dPVgX70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Video and output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T12:04:25.762165Z",
     "iopub.status.busy": "2021-11-03T12:04:25.761858Z",
     "iopub.status.idle": "2021-11-03T12:04:28.032535Z",
     "shell.execute_reply": "2021-11-03T12:04:28.030144Z",
     "shell.execute_reply.started": "2021-11-03T12:04:25.762133Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An example that uses TensorRT's Python api to make inferences.\n",
    "\"\"\"\n",
    "import ctypes\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import tensorrt as trt\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class LoadStreams:  # multiple IP or RTSP cameras\n",
    "    def __init__(self, source='streams.txt', img_size=640, stride=32, auto=True):\n",
    "        self.mode = 'stream'\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "        self.count = 0\n",
    "        self.img_stack, self.imgs, self.fps, self.frames, self.threads = [], None, 0, 0, None\n",
    "        # clean source names for later      清理源文件名称中不支持的字符\n",
    "        self.source = re.sub(pattern=\"[|@#!¡·$€%&()=?¿^*;:,¨´><+]\", repl=\"_\", string=source)\n",
    "        self.auto = auto\n",
    "        # Start thread to read frames from video stream     启动线程从视频流读取帧\n",
    "        print(f'{source}... ', end='')\n",
    "        # i.e. s = '0' local webcam       选择视频流来源是本地摄像头或者输入文件\n",
    "        s = eval(source) if source.isnumeric() else source\n",
    "        # try read frame    尝试读取视频流\n",
    "        cap = cv2.VideoCapture(s)\n",
    "        assert cap.isOpened(), f'Failed to open {s}'\n",
    "        # read w/h/fps/frames in video     读取视频长/宽/帧率/帧数信息\n",
    "        self.width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        # 30 FPS fallback   帧率限幅\n",
    "        self.fps = max(cap.get(cv2.CAP_PROP_FPS) % 100, 0) or 30.0\n",
    "        # infinite stream fallback    帧数限幅\n",
    "        self.frames = max(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float('inf')\n",
    "\n",
    "        _, self.imgs = cap.read()  # guarantee first frame  给imgs传入第一帧，保证开始可以正常读取\n",
    "#         self.img_stack.append(self.imgs)\n",
    "        # Start a separate thread to read the video stream data     启动单独线程读取视频流数据\n",
    "        self.threads = Thread(target=self.update, args=([cap]), daemon=True)\n",
    "        print(f\" success ({self.frames} frames {self.width}x{self.height} at {self.fps:.2f} FPS)\")\n",
    "        self.threads.start()\n",
    "        print('')  # newline\n",
    "\n",
    "    def update(self, cap):\n",
    "        # Read stream frames in daemon thread\n",
    "        n, f, read = 0, self.frames, 1  # frame number, frame array, inference every 'read' frame\n",
    "        while cap.isOpened() and n < f:\n",
    "            n += 1\n",
    "            # _, self.imgs[index] = cap.read()\n",
    "            cap.grab()  # 从视频文件或捕获设备获取下一帧\n",
    "            if n % read == 0:\n",
    "                success, im = cap.retrieve()  # 解码并返回抓取了的视频帧\n",
    "                if success:\n",
    "                    self.imgs = im\n",
    "                    self.img_stack.append(self.imgs)\n",
    "                else:\n",
    "                    self.imgs = self.imgs * 0\n",
    "            time.sleep(1 / self.fps)  # wait time\n",
    "\n",
    "    def __iter__(self):\n",
    "        #         self.count = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.count += 1\n",
    "        while len(self.img_stack) == 0:\n",
    "            time.sleep(1 / self.fps)\n",
    "        img0 = self.img_stack.pop(0)\n",
    "        img = np.array(img0)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)  # 1E12 frames = 32 streams at 30 FPS for 30 years\n",
    "\n",
    "    \n",
    "CONF_THRESH = 0.5\n",
    "IOU_THRESHOLD = 0.4\n",
    "\n",
    "def time_sync():\n",
    "    # pytorch-accurate time\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    return time.time()\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    \"\"\"\n",
    "    description: Plots one bounding box on image img,\n",
    "                 this function comes from YoLov5 project.\n",
    "    param:\n",
    "        x:      a box likes [x1,y1,x2,y2]\n",
    "        img:    a opencv image object\n",
    "        color:  color to draw rectangle, such as (0,255,0)\n",
    "        label:  str\n",
    "        line_thickness: int\n",
    "    return:\n",
    "        no return\n",
    "\n",
    "    \"\"\"\n",
    "    tl = (\n",
    "            line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    )  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            label,\n",
    "            (c1[0], c1[1] - 2),\n",
    "            0,\n",
    "            tl / 3,\n",
    "            [225, 255, 255],\n",
    "            thickness=tf,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "\n",
    "class YoLov5TRT(object):\n",
    "    \"\"\"\n",
    "    description: A YOLOv5 class that warps TensorRT ops, preprocess and postprocess ops.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, engine_file_path):\n",
    "        self.padding_size = {}\n",
    "\n",
    "        # Create a Context on this device.\n",
    "        # Attention: Here Cuda's context is different with below tensorrt context!!!!\n",
    "        # 创建Cuda的context。它非常重要，它作为一个容器，管理了所有对象的生命周期。\n",
    "        # 注意:这里Cuda的Context与下面tensorrt的Context是不同的\n",
    "        self.ctx = cuda.Device(0).make_context()\n",
    "        # Manage the concurrency of execution units.    管理执行单元的并发性\n",
    "        # CUDA流表示一个GPU操作队列，该队列中的操作将以添加到流中的先后顺序而依次执行。\n",
    "        # 可以将一个流看做是GPU上的一个任务，不同任务可以并行执行。\n",
    "        stream = cuda.Stream()\n",
    "        # Create Tensort engine.    创建Tensorrt引擎\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "        self.runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "        # Deserialize the engine from file.  对文件中的引擎进行反序列化，换句话说，就是解析本地engine文件。\n",
    "        with open(engine_file_path, \"rb\") as f:\n",
    "            engine = self.runtime.deserialize_cuda_engine(f.read())\n",
    "        # Create an IExecutionContext.\n",
    "        # 创建一个Engine的Context。Context是真正推理的入口，其储存了中间值，一个Engine允许多个Context。\n",
    "        context = engine.create_execution_context()\n",
    "\n",
    "        host_inputs = []\n",
    "        cuda_inputs = []\n",
    "        host_outputs = []\n",
    "        cuda_outputs = []\n",
    "        bindings = []\n",
    "\n",
    "        # choose bindings from engine, each binding handle one batch\n",
    "        # 选取引擎的各个端口，每个端口处理一个batch。端口的具体数值其实是内存地址，即 int(buffer)\n",
    "        for binding in engine:\n",
    "            # Get the shape of a binding.   得到端口的传输特征。\n",
    "            print('bingding:', binding, engine.get_binding_shape(binding))\n",
    "            # Computes the volume of an iterable.   计算可迭代对象的总容量。\n",
    "            size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "            # Returns the numpy-equivalent of a TensorRT DataType. 返回TensorRT数据类型的numpy等效值。\n",
    "            dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "\n",
    "            # Allocate host and device buffers.     分配主机缓冲区和GPU缓冲区\n",
    "            # CPU data allocations are pageable by default. The GPU cannot access data directly from pageable host memory,\n",
    "            # so when a data transfer from pageable host memory to device memory is invoked,\n",
    "            # the CUDA driver must first allocate a temporary page-locked, or “pinned”, host array,\n",
    "            # copy the data to the pinned array, and then transfer the data from the pinned array to device memory.\n",
    "            # 默认情况下，CPU数据分配是可分页的，GPU不能直接从可分页内存(host memory)访问数据。\n",
    "            # 因此当从可分页内存到显存(device memory)的数据传输被调用时，CUDA驱动程序必须首先分配一个临时的页面锁定，\n",
    "            # 或“固定”主机阵列，用来将内存数据复制到固定的数组，然后再从固定数组转移到显存。\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            cuda_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "\n",
    "            # Append the device buffer to device bindings.      将GPU缓冲区附加到GPU端口。\n",
    "            bindings.append(int(cuda_mem))\n",
    "            # Append to the appropriate list.       将缓冲区添加到合适的缓冲队列。\n",
    "            if engine.binding_is_input(binding):\n",
    "                self.input_w = engine.get_binding_shape(binding)[-1]\n",
    "                self.input_h = engine.get_binding_shape(binding)[-2]\n",
    "                host_inputs.append(host_mem)\n",
    "                cuda_inputs.append(cuda_mem)\n",
    "            else:\n",
    "                host_outputs.append(host_mem)\n",
    "                cuda_outputs.append(cuda_mem)\n",
    "\n",
    "        # Store\n",
    "        self.stream = stream\n",
    "        self.context = context\n",
    "        self.engine = engine\n",
    "        self.host_inputs = host_inputs\n",
    "        self.cuda_inputs = cuda_inputs\n",
    "        self.host_outputs = host_outputs\n",
    "        self.cuda_outputs = cuda_outputs\n",
    "        self.bindings = bindings\n",
    "        self.batch_size = engine.max_batch_size\n",
    "\n",
    "\n",
    "    def infer(self, raw_image_generator, categories, format='mp4v'):\n",
    "        # save_name     保存地址\n",
    "        parent, filename = os.path.split(raw_image_generator.source)\n",
    "        save_path = f\"{time.strftime('%H_%M_%S__')}{filename}\"\n",
    "        # Create video writer   创建视频写入对象\n",
    "        self.vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*format),\n",
    "                                          raw_image_generator.fps, \n",
    "                                          (raw_image_generator.width, raw_image_generator.height))\n",
    "        # calculate padding size    计算填充尺寸\n",
    "        self.Video_padding_size(raw_image_generator.width, raw_image_generator.height)\n",
    "        print(raw_image_generator.width, raw_image_generator.height, '\\n', self.input_w, self.input_h)\n",
    "        self.categories = categories\n",
    "\n",
    "        # Set batch     设置batch\n",
    "        process_num = raw_image_generator.frames // self.batch_size\n",
    "        print(self.batch_size * process_num, process_num)\n",
    "        for s in range(process_num):\n",
    "            # Make self the active context, pushing it on top of the context stack.\n",
    "            # 使self成为活动context，将它推到Cuda处理context堆栈的顶部。\n",
    "            self.ctx.push()\n",
    "            # Restore   恢复\n",
    "            stream = self.stream\n",
    "            context = self.context\n",
    "            engine = self.engine\n",
    "            host_inputs = self.host_inputs\n",
    "            cuda_inputs = self.cuda_inputs\n",
    "            host_outputs = self.host_outputs\n",
    "            cuda_outputs = self.cuda_outputs\n",
    "            bindings = self.bindings\n",
    "            # Do image preprocess   进行图像处理\n",
    "            batch_image_raw = []\n",
    "            batch_origin_h = []\n",
    "            batch_origin_w = []\n",
    "            batch_input_image = np.empty(shape=[self.batch_size, 3, self.input_h, self.input_w])\n",
    "            for i in range(self.batch_size):\n",
    "                image_raw = next(raw_image_generator)\n",
    "                input_image, image_raw, origin_h, origin_w = self.preprocess_image(image_raw)\n",
    "                batch_image_raw.append(image_raw)\n",
    "                batch_origin_h.append(origin_h)\n",
    "                batch_origin_w.append(origin_w)\n",
    "                np.copyto(batch_input_image[i], input_image)\n",
    "            batch_input_image = np.ascontiguousarray(batch_input_image)\n",
    "            print('frame_count:', raw_image_generator.count, end=', ')\n",
    "\n",
    "            # Copy input image to host buffer       复制输入图像到内存\n",
    "            np.copyto(host_inputs[0], batch_input_image.ravel())\n",
    "            start = time_sync()\n",
    "            # Transfer input data to the GPU.      将输入数据传输到GPU\n",
    "            cuda.memcpy_htod_async(cuda_inputs[0], host_inputs[0], stream)\n",
    "            # Run inference.        进行推理\n",
    "            context.execute_async(batch_size=self.batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "            # Transfer predictions back from the GPU.       从GPU传回预测结果\n",
    "            cuda.memcpy_dtoh_async(host_outputs[0], cuda_outputs[0], stream)\n",
    "            # Synchronize the stream        同步流\n",
    "#             stream.synchronize()\n",
    "            end = time_sync()\n",
    "            # Remove any context from the top of the context stack, deactivating it.\n",
    "            # 从Cuda context堆栈的顶部移除所有context，使其失效。\n",
    "            self.ctx.pop()\n",
    "            # Here we use the first row of output in that batch_size = 1\n",
    "            output = host_outputs[0]\n",
    "            batch_time = (end - start) * 1000\n",
    "            # Do postprocess        进行后处理（包括框格标定等）\n",
    "            for i in range(self.batch_size):\n",
    "                cv2.putText(batch_image_raw[i], f'batch_fps:{str(int(self.batch_size / batch_time * 1000))}', \n",
    "                            (20, 50), 0, (raw_image_generator.width + raw_image_generator.height)*0.002/6, \n",
    "                            [10, 10, 10], thickness=4, lineType=cv2.LINE_AA)\n",
    "                result_boxes, result_scores, result_classid = self.post_process(\n",
    "                    output[i * 6001: (i + 1) * 6001], batch_origin_h[i], batch_origin_w[i]\n",
    "                )\n",
    "                # Draw rectangles and labels on the original image\n",
    "                for j in range(len(result_boxes)):\n",
    "                    box = result_boxes[j]\n",
    "                    plot_one_box(box, batch_image_raw[i], \n",
    "                                 label=\"{}:{:.2f}\".format(\n",
    "                                     self.categories[int(result_classid[j])], result_scores[j]),)\n",
    "                raw = np.array(batch_image_raw[i])\n",
    "                # raw: HWC!!! Not WHC!!!                  \n",
    "                self.vid_writer.write(raw)\n",
    "            print('input->{}, time->{:.2f}ms, per_time->{:.2f}ms, saving into '\n",
    "                  .format(s, batch_time, batch_time / self.batch_size) + save_path)\n",
    "        self.vid_writer.release()  # release previous video writer\n",
    "\n",
    "\n",
    "    def Video_padding_size(self, w, h):\n",
    "        \"\"\"\n",
    "        Here calculate width and height and paddings\n",
    "\n",
    "        Args:\n",
    "            w: Input video width\n",
    "            h: Input video height\n",
    "\n",
    "        Returns: dict of padding size\n",
    "\n",
    "        \"\"\"\n",
    "        r_w = self.input_w / w\n",
    "        r_h = self.input_h / h\n",
    "        if r_h > r_w:\n",
    "            tw = self.input_w\n",
    "            th = int(r_w * h)\n",
    "            tx1 = tx2 = 0\n",
    "            ty1 = int((self.input_h - th) / 2)\n",
    "            ty2 = self.input_h - th - ty1\n",
    "        else:\n",
    "            tw = int(r_h * w)\n",
    "            th = self.input_h\n",
    "            tx1 = int((self.input_w - tw) / 2)\n",
    "            tx2 = self.input_w - tw - tx1\n",
    "            ty1 = ty2 = 0\n",
    "\n",
    "        self.padding_size = {'tw': tw,\n",
    "                             'th': th,\n",
    "                             'tx1': tx1,\n",
    "                             'tx2': tx2,\n",
    "                             'ty1': ty1,\n",
    "                             'ty2': ty2}\n",
    "\n",
    "    def preprocess_image(self, raw_bgr_image):\n",
    "        \"\"\"\n",
    "        description: Convert BGR image to RGB,\n",
    "                     resize and pad it to target size, normalize to [0,1],\n",
    "                     transform to NCHW format.\n",
    "        param:\n",
    "            input_image_path: str, image path\n",
    "        return:\n",
    "            image:  the processed image\n",
    "            image_raw: the original image\n",
    "            h: original height\n",
    "            w: original width\n",
    "        \"\"\"\n",
    "        image_raw = raw_bgr_image\n",
    "        pad = self.padding_size\n",
    "\n",
    "        h, w, c = image_raw.shape\n",
    "        image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "        # Resize the image with long side while maintaining ratio\n",
    "        image = cv2.resize(image, (pad['tw'], pad['th']))\n",
    "        # Pad the short side with (128,128,128)\n",
    "        image = cv2.copyMakeBorder(\n",
    "            image, pad['ty1'], pad['ty2'], pad['tx1'], pad['tx2'],\n",
    "            cv2.BORDER_CONSTANT, (128, 128, 128)\n",
    "        )\n",
    "        image = image.astype(np.float32)\n",
    "        # Normalize to [0,1]\n",
    "        image /= 255.0\n",
    "        # HWC to CHW format:\n",
    "        image = np.transpose(image, [2, 0, 1])\n",
    "        # CHW to NCHW format\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        # Convert the image to row-major order, also known as \"C order\":\n",
    "        image = np.ascontiguousarray(image)\n",
    "        return image, image_raw, h, w\n",
    "\n",
    "    def xywh2xyxy(self, origin_h, origin_w, x):\n",
    "        \"\"\"\n",
    "        description:    Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "        param:\n",
    "            origin_h:   height of original image\n",
    "            origin_w:   width of original image\n",
    "            x:          A boxes numpy, each row is a box [center_x, center_y, w, h]\n",
    "        return:\n",
    "            y:          A boxes numpy, each row is a box [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        y = np.zeros_like(x)\n",
    "        r_w = self.input_w / origin_w\n",
    "        r_h = self.input_h / origin_h\n",
    "        if r_h > r_w:\n",
    "            y[:, 0] = x[:, 0] - x[:, 2] / 2\n",
    "            y[:, 2] = x[:, 0] + x[:, 2] / 2\n",
    "            y[:, 1] = x[:, 1] - x[:, 3] / 2 - (self.input_h - r_w * origin_h) / 2\n",
    "            y[:, 3] = x[:, 1] + x[:, 3] / 2 - (self.input_h - r_w * origin_h) / 2\n",
    "            y /= r_w\n",
    "        else:\n",
    "            y[:, 0] = x[:, 0] - x[:, 2] / 2 - (self.input_w - r_h * origin_w) / 2\n",
    "            y[:, 2] = x[:, 0] + x[:, 2] / 2 - (self.input_w - r_h * origin_w) / 2\n",
    "            y[:, 1] = x[:, 1] - x[:, 3] / 2\n",
    "            y[:, 3] = x[:, 1] + x[:, 3] / 2\n",
    "            y /= r_h\n",
    "\n",
    "        return y\n",
    "\n",
    "    def post_process(self, output, origin_h, origin_w):\n",
    "        \"\"\"\n",
    "        description: postprocess the prediction\n",
    "        param:\n",
    "            output:     A numpy likes [num_boxes,cx,cy,w,h,conf,cls_id, cx,cy,w,h,conf,cls_id, ...]\n",
    "            origin_h:   height of original image\n",
    "            origin_w:   width of original image\n",
    "        return:\n",
    "            result_boxes: finally boxes, a boxes numpy, each row is a box [x1, y1, x2, y2]\n",
    "            result_scores: finally scores, a numpy, each element is the score correspoing to box\n",
    "            result_classid: finally classid, a numpy, each element is the classid correspoing to box\n",
    "        \"\"\"\n",
    "        # Get the num of boxes detected\n",
    "        num = int(output[0])\n",
    "        # Reshape to a two dimentional ndarray\n",
    "        pred = np.reshape(output[1:], (-1, 6))[:num, :]\n",
    "        # Do nms\n",
    "        boxes = self.non_max_suppression(pred, origin_h, origin_w, conf_thres=CONF_THRESH, nms_thres=IOU_THRESHOLD)\n",
    "        result_boxes = boxes[:, :4] if len(boxes) else np.array([])\n",
    "        result_scores = boxes[:, 4] if len(boxes) else np.array([])\n",
    "        result_classid = boxes[:, 5] if len(boxes) else np.array([])\n",
    "        return result_boxes, result_scores, result_classid\n",
    "\n",
    "    def bbox_iou(self, box1, box2, x1y1x2y2=True):\n",
    "        \"\"\"\n",
    "        description: compute the IoU of two bounding boxes\n",
    "        param:\n",
    "            box1: A box coordinate (can be (x1, y1, x2, y2) or (x, y, w, h))\n",
    "            box2: A box coordinate (can be (x1, y1, x2, y2) or (x, y, w, h))\n",
    "            x1y1x2y2: select the coordinate format\n",
    "        return:\n",
    "            iou: computed iou\n",
    "        \"\"\"\n",
    "        if not x1y1x2y2:\n",
    "            # Transform from center and width to exact coordinates\n",
    "            b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "            b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "            b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "            b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "        else:\n",
    "            # Get the coordinates of bounding boxes\n",
    "            b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "            b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "        # Get the coordinates of the intersection rectangle\n",
    "        inter_rect_x1 = np.maximum(b1_x1, b2_x1)\n",
    "        inter_rect_y1 = np.maximum(b1_y1, b2_y1)\n",
    "        inter_rect_x2 = np.minimum(b1_x2, b2_x2)\n",
    "        inter_rect_y2 = np.minimum(b1_y2, b2_y2)\n",
    "        # Intersection area\n",
    "        inter_area = np.clip(inter_rect_x2 - inter_rect_x1 + 1, 0, None) * \\\n",
    "                     np.clip(inter_rect_y2 - inter_rect_y1 + 1, 0, None)\n",
    "        # Union Area\n",
    "        b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "        b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def non_max_suppression(self, prediction, origin_h, origin_w, conf_thres=0.5, nms_thres=0.4):\n",
    "        \"\"\"\n",
    "        description: Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "        Non-Maximum Suppression to further filter detections.\n",
    "        param:\n",
    "            prediction: detections, (x1, y1, x2, y2, conf, cls_id)\n",
    "            origin_h: original image height\n",
    "            origin_w: original image width\n",
    "            conf_thres: a confidence threshold to filter detections\n",
    "            nms_thres: a iou threshold to filter detections\n",
    "        return:\n",
    "            boxes: output after nms with the shape (x1, y1, x2, y2, conf, cls_id)\n",
    "        \"\"\"\n",
    "        # Get the boxes that score > CONF_THRESH\n",
    "        boxes = prediction[prediction[:, 4] >= conf_thres]\n",
    "        # Trandform bbox from [center_x, center_y, w, h] to [x1, y1, x2, y2]\n",
    "        boxes[:, :4] = self.xywh2xyxy(origin_h, origin_w, boxes[:, :4])\n",
    "        # clip the coordinates\n",
    "        boxes[:, 0] = np.clip(boxes[:, 0], 0, origin_w - 1)\n",
    "        boxes[:, 2] = np.clip(boxes[:, 2], 0, origin_w - 1)\n",
    "        boxes[:, 1] = np.clip(boxes[:, 1], 0, origin_h - 1)\n",
    "        boxes[:, 3] = np.clip(boxes[:, 3], 0, origin_h - 1)\n",
    "        # Object confidence\n",
    "        confs = boxes[:, 4]\n",
    "        # Sort by the confs\n",
    "        boxes = boxes[np.argsort(-confs)]\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        while boxes.shape[0]:\n",
    "            large_overlap = self.bbox_iou(np.expand_dims(boxes[0, :4], 0), boxes[:, :4]) > nms_thres\n",
    "            label_match = boxes[0, -1] == boxes[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            keep_boxes += [boxes[0]]\n",
    "            boxes = boxes[~invalid]\n",
    "        boxes = np.stack(keep_boxes, 0) if len(keep_boxes) else np.array([])\n",
    "        return boxes\n",
    "\n",
    "    def destroy(self):\n",
    "        # Remove any context from the top of the context stack, deactivating it.\n",
    "        self.ctx.pop()\n",
    "#         self.ctx.destory()\n",
    "#         del self.ctx\n",
    "\n",
    "\n",
    "class inferThread:\n",
    "    def __init__(self, yolov5_wrapper, src_path, categories):\n",
    "        #         threading.Thread.__init__(self)\n",
    "        self.yolov5_wrapper = yolov5_wrapper\n",
    "        self.src_path = src_path\n",
    "        self.categories = categories\n",
    "\n",
    "    def run(self):\n",
    "        # Load stream       加载视频流\n",
    "        raw_image_generator = LoadStreams(source=self.src_path)\n",
    "        self.yolov5_wrapper.infer(raw_image_generator, self.categories)\n",
    "\n",
    "\n",
    "%cd /kaggle/working\n",
    "# load custom plugin and engine\n",
    "PLUGIN_LIBRARY = \"./tensorrtx/yolov5/build/libmyplugins.so\"\n",
    "engine_file_path = \"./best.engine\"\n",
    "src_file_path = \"./001.mp4\"\n",
    "\n",
    "# load labels\n",
    "categories = ['A10','A400M', 'AG600','B1','B2','B52','Be200', 'C130','C17','C5', 'E2','EF2000',\n",
    "    'F117','F14','F15','F16','F18','F22','F35','F4',\n",
    "    'JAS39','MQ9','Mig31','Mirage2000','RQ4','Rafale',\n",
    "    'SR71','Su57','Tu160','Tu95','U2','US2', 'V22','XB70','YF23','J20']\n",
    "\n",
    "# load plugins\n",
    "ctypes.CDLL(PLUGIN_LIBRARY)\n",
    "# a YoLov5TRT instance\n",
    "yolov5_wrapper = YoLov5TRT(engine_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T12:04:31.836629Z",
     "iopub.status.busy": "2021-11-03T12:04:31.836217Z",
     "iopub.status.idle": "2021-11-03T12:06:16.087753Z",
     "shell.execute_reply": "2021-11-03T12:06:16.086928Z",
     "shell.execute_reply.started": "2021-11-03T12:04:31.83659Z"
    }
   },
   "outputs": [],
   "source": [
    "print('batch size is', yolov5_wrapper.batch_size)\n",
    "infer = inferThread(yolov5_wrapper, src_file_path, categories)\n",
    "infer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detach resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-03T11:49:44.727203Z",
     "iopub.status.busy": "2021-11-03T11:49:44.72652Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实例释放\n",
    "yolov5_wrapper.ctx.detach() \n",
    "# yolov5_wrapper.ctx\n",
    "del yolov5_wrapper.host_inputs\n",
    "del yolov5_wrapper.host_outputs\n",
    "del yolov5_wrapper.cuda_inputs\n",
    "del yolov5_wrapper.cuda_outputs\n",
    "del yolov5_wrapper.stream\n",
    "del yolov5_wrapper.context\n",
    "del yolov5_wrapper.engine\n",
    "del yolov5_wrapper.bindings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
